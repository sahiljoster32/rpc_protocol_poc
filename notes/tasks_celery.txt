1. Setup multiple consumers single producer. (Done)
    1.1. Celey is used to add tasks in queue.
    1.2. only specific tasks should be executed by consumer1 and same goes for consumer2.
    1.3. proucer can add tasks inside queues for both consumer.
    1.4. everything should be working with docker enviroment.
2. Setup all logs inside a file. (All logs should be written into a single file/folder, for now in folder -- per app) - docker pv.
    2.1. Can save all logs inside a centeral PV.
    2.2. created central pv, and used bind volume which uses host machine's log folder.
    2.2. Std out is ok but can we store error in much more descriptive way ? like adding timestamp, ip, user name & extras.
3. K8s pai deploy karne ke bare mai sochna.
    3.1. deployed on k8s cluster using deployments and services & replicasets & selectors objects.
    3.2. Implement ingress for accessing the whole app from external. -- currently only accessing from inside because no nodeport is defined.
    3.3. Implement the pv volume and pvc for storing all logs in single place.
4. SSH the logs server.
    4.1. Implement SSH for container.
    4.2. Do SCP or file transfere to check if it is working or not.
    4.3. Once SS is done, try to run a script inside it (even simple file change could do the work).
5. Celery parallel task execution.
6. Binary format data transfere instead of normal json fomrat.
7. supervisord.confg node also make so that we know, which node is down ?!
****************